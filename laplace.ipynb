{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we present the code used for the numerical simulations in Chapter 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.integrate as integrate \n",
    "import matplotlib.pyplot as plt\n",
    "from pymor.basic import *\n",
    "\n",
    "from pymor.core.logger import set_log_levels\n",
    "set_log_levels({'pymor': 'ERROR'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hat functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hat(x,i,I):\n",
    "    h = I[1]-I[0]\n",
    "    N = len(I)\n",
    "    if i == 0:\n",
    "        if x < I[i]:\n",
    "            return 0\n",
    "        elif x <= I[i+1]:\n",
    "            return -(x-I[i+1])/h \n",
    "        else:\n",
    "            return 0\n",
    "    elif i <= N-2:\n",
    "        if x < I[i-1]:\n",
    "            return 0\n",
    "        elif x <= I[i]:\n",
    "            return (x-I[i-1])/h \n",
    "        elif x <= I[i+1]:\n",
    "            return -(x-I[i+1])/h \n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if x < I[i-1]:\n",
    "            return 0 \n",
    "        elif x <= I[i]:\n",
    "            return (x-I[i-1])/h \n",
    "        else:\n",
    "           return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now simulate \n",
    "$\\begin{align*}\n",
    "-\\Delta u(x) - L^{\\alpha}u(x) &= \\sin(x),\\quad x\\in (-2\\pi,2\\pi),\\\\\n",
    "u(-2\\pi) = u(2\\pi) &= 0.\n",
    "\\end{align*}$\n",
    "This problem is well posed on $H^1_0(\\Omega)$ for each $\\alpha \\in(0,1)$. We take the $H^1$ seminorm which is a norm on $H^1_0$ equivalent to the usual Sobolev norm. Using this norm the inner product matrix is equal to the stiffnes matrix associated to the bilinearform.\\\n",
    "We begin by setting up the stiffnes matrix for the Laplace operator:\n",
    "$\\begin{align*}\n",
    "a(\\phi_i,\\phi_j)=\\int_a^b \\phi_i'\\phi_j' dx\n",
    "\\end{align*}$\n",
    "by definition this is equal to the $H^1_0$ seminorm, i.e. we can use both for pyMOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the stiffnes matrix for the laplacian with homogenous boundary conditions ($u=0$ on $\\partial\\Omega$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H1_semiprod(I):\n",
    "    N = len(I)\n",
    "    h = I[1]-I[0]\n",
    "    diag1 = np.ones(N-2)\n",
    "    diag2 = np.ones(N-3)\n",
    "    A = np.diagflat(2*diag1,0) + np.diagflat(-diag2,1) +np.diagflat(-diag2,-1)\n",
    "    return (1/h)*A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now determine the stiffnes matrix for $L^{\\alpha}$. In view of the readability of the code we define some auxilliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def innerint_1(y,i,I,alpha):\n",
    "    return integrate.quad(lambda x: hat(x,i,I)*(y-x)**(-alpha),I[i],y)[0]+integrate.quad(lambda x: hat(x,i,I)*(x-y)**(-alpha),y,I[i+1])[0]\n",
    "\n",
    "def innerint_2(y,i,I,alpha):\n",
    "    val = integrate.quad(lambda x: hat(x,i+1,I)*(y-x)**(-alpha),I[i],y)[0]+integrate.quad(lambda x: hat(x,i+1,I)*(x-y)**(-alpha),y,I[i+1])[0]\n",
    "    return val\n",
    "\n",
    "def seconddiag(i,I,alpha):\n",
    "    return integrate.quad(lambda y: hat(y,i,I)*innerint_2(y,i,I,alpha),I[i],I[i+1])[0]\\\n",
    "                        + integrate.dblquad(lambda x,y: hat(y,i,I)*hat(x,i+1,I)*(x-y)**(-alpha),I[i],I[i+1],I[i+1],I[i+2])[0]\\\n",
    "                            +integrate.dblquad(lambda x,y: hat(y,i,I)*hat(x,i+1,I)*(x-y)**(-alpha),I[i-1],I[i],I[i],I[i+2])[0]\n",
    "                    \n",
    "def firstdiag(i,I,alpha):\n",
    "    i2 = integrate.dblquad(lambda x,y: hat(y,i,I)*hat(x,i,I)*(x-y)**(-alpha),I[i-1],I[i],I[i],I[i+1])[0]\n",
    "    i3 = integrate.dblquad(lambda x,y: hat(y,i,I)*hat(x,i,I)*(y-x)**(-alpha),I[i],I[i+1],I[i-1],I[i])[0]\n",
    "    i4 = integrate.quad(lambda y: hat(y,i,I)*innerint_1(y,i,I,alpha),I[i],I[i+1])[0]\n",
    "    i1 = integrate.quad(lambda y: hat(y,i,I)*innerint_2(y,i-1,I,alpha),I[i-1],I[i])[0]\n",
    "    return i1+i2+i3+i4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stiff1(I,alpha):   \n",
    "\n",
    "    N = len(I)\n",
    "    B = np.zeros((N,N))\n",
    "    val0 = firstdiag(1,I,alpha)\n",
    "    val1 = seconddiag(1,I,alpha)\n",
    "    \n",
    "    for i in range(N-1):\n",
    "            if i == 0:\n",
    "                B[i,i] = (1/(1-alpha))*integrate.quad(lambda y: hat(y,i,I)**2*((y-I[0])**(1-alpha)+(I[-1]-y)**(1-alpha)),I[0],I[1])[0]\\\n",
    "                    - integrate.quad(lambda y: hat(y,0,I)*innerint_1(y,0,I,alpha),I[0],I[1])[0]\n",
    "                B[i,i+1] = (1/(1-alpha))*integrate.quad(lambda y: hat(y,i,I)*hat(y,i+1,I)*((y-I[0])**(1-alpha)+(I[-1]-y)**(1-alpha)),I[0],I[1])[0]\\\n",
    "                    -(integrate.quad(lambda y: hat(y,0,I)*innerint_2(y,0,I,alpha),I[0],I[1])[0]\\\n",
    "                        + integrate.dblquad(lambda x,y: hat(y,0,I)*hat(x,1,I)*(x-y)**(-alpha),I[0],I[1],I[1],I[2])[0])\n",
    "                for j in range(2,N-1):\n",
    "                    B[i,j] = -integrate.dblquad(lambda x,y: hat(y,i,I)*hat(x,j,I)*(x-y)**(-alpha),I[0],I[1],I[j-1],I[j+1])[0]\n",
    "                B[i,N-1] = -integrate.dblquad(lambda x,y: hat(y,i,I)*hat(x,N-1,I)*(x-y)**(-alpha),I[0],I[1],I[N-2],I[N-1])[0]\n",
    "            elif i == 1:\n",
    "                B[i,i] = (1/(1-alpha))*integrate.quad(lambda y: hat(y,i,I)**2*((y-I[0])**(1-alpha)+(I[-1]-y)**(1-alpha)),I[i-1],I[i+1])[0]\\\n",
    "                        -val0\n",
    "                B[i,i+1] = (1/(1-alpha))*integrate.quad(lambda y: hat(y,i,I)*hat(y,i+1,I)*((y-I[0])**(1-alpha)+(I[-1]-y)**(1-alpha)),I[i],I[i+1])[0]\\\n",
    "                       - val1\n",
    "                for j in range(3,N-1):\n",
    "                    B[i,j] = -integrate.dblquad(lambda x,y: hat(y,i,I)*hat(x,j,I)*(x-y)**(-alpha),I[i-1],I[i+1],I[j-1],I[j+1])[0]\n",
    "                B[i,N-1] = -integrate.dblquad(lambda x,y: hat(y,i,I)*hat(x,N-1,I)*(x-y)**(-alpha),I[0],I[2],I[N-2],I[N-1])[0]\n",
    "            else:\n",
    "                B[i,i] = (1/(1-alpha))*integrate.quad(lambda y: hat(y,i,I)**2*((y-I[0])**(1-alpha)+(I[-1]-y)**(1-alpha)),I[i-1],I[i+1])[0]\\\n",
    "                    -val0\n",
    "                B[i,i+1] = (1/(1-alpha))*integrate.quad(lambda y: hat(y,i,I)*hat(y,i+1,I)*((y-I[0])**(1-alpha)+(I[-1]-y)**(1-alpha)),I[i],I[i+1])[0]\\\n",
    "                    -val1\n",
    "\n",
    "            B[N-2,N-1] = B[0,1]       \n",
    "            B[N-1,N-1] = B[0,0]\n",
    "\n",
    "    for i in range(N):\n",
    "            if i == 0 or i == 1:\n",
    "                continue\n",
    "            elif i == N-2:\n",
    "                continue\n",
    "            else:\n",
    "                B[i,i+2:] = B[i-1,i+1:N-1]\n",
    "\n",
    "    B = B + B.T - np.diag(np.diagonal(B))\n",
    "    return(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the right hand-side vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rhs(I): \n",
    "\n",
    "    N = len(I)\n",
    "    rhs = np.zeros((1,N-2)).T\n",
    "\n",
    "    for i in range(1,N-1):\n",
    "        if i == 0:\n",
    "            rhs[i] = 0\n",
    "        elif i == N-1:\n",
    "            rhs[i] = 0\n",
    "        else:\n",
    "            rhs[i-1] = integrate.quad(lambda x: hat(x,i,I)*(np.sin(x)),I[i-1],I[i+1])[0]\n",
    "    return rhs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we investigate the convergence rate of the simulated solution against the analytic solution, plotted in the $H^1$ semi-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation the squared H^1_0 error for sinus\n",
    "def H1_0err(u_h,I):\n",
    "    h = I[1]-I[0]\n",
    "    err = 0\n",
    "    for k in range(len(I)-1):\n",
    "        tmp = (u_h[k+1] - u_h[k])/h\n",
    "        err += (1/4)*(-2*I[k]+2*I[k+1]-np.sin(2*I[k])+np.sin(2*I[k+1])) - 2*tmp*(np.sin(I[k+1])-np.sin(I[k])) + tmp**2*h\n",
    "    print(err)\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_range = np.array([8,16,32,64,128,256,512,1024,2048,4096,8192,16384,32768])\n",
    "k = 0\n",
    "err_H1 = np.zeros(N_range.shape)\n",
    "for N in N_range:\n",
    "    N = int(N)\n",
    "    I_N = np.linspace(-2*np.pi,2*np.pi,N)\n",
    "    u_h = np.zeros((N,1))\n",
    "    A = H1_semiprod(I_N)\n",
    "    rhs = set_rhs(I_N)\n",
    "    u_h[1:N-1] = scipy.linalg.solve(A,rhs,sym_pos=True)\n",
    "    err_H1[k] = H1_0err(u_h,I_N)\n",
    "    k += 1\n",
    "\n",
    "plt.figure(0)\n",
    "plt.loglog(N_range,err_H1,'-x',label ='$H^1_0$ error')\n",
    "plt.grid(True)\n",
    "plt.xlabel('$\\mathcal{N}$')\n",
    "plt.ylabel('$||u-u_h||_{H^1_0}^2$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start investigating the reducibility of the problem. In the thesis, we first investigated the reducibility for $\\alpha = 0.2$. Then we investigated if there is a connection between the reducibility, and the order of singularity.\\\n",
    " The code for those experiments is the same, one only has to define for which $\\alpha$'s the model should be considered, stored in the vector 'alpha'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.linspace(-2*np.pi,2*np.pi,2000)\n",
    "A1 = H1_semiprod(I)\n",
    "N = len(I)\n",
    "\n",
    "#define the singularity order for which the model should be considered.\n",
    "alpha = [0.2,0.4,0.6,0.7,0.85]\n",
    "\n",
    "A2 = []\n",
    "for a in alpha:\n",
    "    tmp = stiff1(I,a)\n",
    "    A2.append(tmp[1:N-1,1:N-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the problem in pyMOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "H10_prod = NumpyMatrixOperator(H1_semiprod(I))\n",
    "\n",
    "rhs = set_rhs(I)\n",
    "rhs_op = NumpyMatrixOperator(rhs)\n",
    "\n",
    "coefs = [ProjectionParameterFunctional('mu', 2,0),\n",
    "            ProjectionParameterFunctional('mu',2,1)]\n",
    "\n",
    "fom = []\n",
    "for i,a in enumerate(alpha):\n",
    "    ops = [NumpyMatrixOperator(A1),NumpyMatrixOperator(A2[i])]\n",
    "    op = LincombOperator(ops,coefs)\n",
    "    fom.append(StationaryModel(op,rhs_op))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the approximated solution for some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0 = plt.figure(0)\n",
    "for i,a in enumerate(alpha):\n",
    "    U = fom[i].solve(Mu({'mu':[0.5,0.5]}))\n",
    "    u = U.to_numpy()[0]\n",
    "    plt.plot(I[1:N-1],u,label = r'$\\alpha =$ %.2f' %(a))\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a coerive reductor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reductor = []\n",
    "for i,a in enumerate(alpha):\n",
    "    reductor.append(CoerciveRBReductor(\n",
    "                        fom[i],\n",
    "                        product = H10_prod,\n",
    "                ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the reduced basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 300\n",
    "atol = 1e-10\n",
    "N_max = 50\n",
    "parameter_space = []\n",
    "greedy_data = []\n",
    "\n",
    "\n",
    "low_bounds = [0.001,1]\n",
    "upp_bounds = [0.001,1]\n",
    "par_bounds = {'mu':low_bounds,'mu':upp_bounds}\n",
    "train_sample= fom[0].parameters.space(par_bounds).sample_randomly(n_train)\n",
    "\n",
    "for i,a in enumerate(alpha):\n",
    "    parameter_space.append(fom[i].parameters.space(0.001,1))\n",
    "    greedy_data.append(rb_greedy(fom[i], reductor[i], train_sample,\n",
    "                                max_extensions=N_max, atol=atol))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_err = []\n",
    "for i,a in enumerate(alpha):\n",
    "    train_err.append(greedy_data[i]['max_errs'])\n",
    "\n",
    "for i,a in enumerate(alpha):\n",
    "    plt.semilogy(range(greedy_data[i]['extensions']+1),train_err[i], label = r'$\\alpha =$ %.2f' %(a))\n",
    "plt.legend()\n",
    "plt.xlabel('$N$')\n",
    "plt.ylabel('$max_{\\mu} ||u_h(\\mu)-u_N(\\mu)||$')\n",
    "plt.grid(which='both')\n",
    "plt.title('Training error in  $|\\cdot|_{H^1_0}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the reduced models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom = []\n",
    "bases = []\n",
    "for i,a in enumerate(alpha):\n",
    "    rom.append(reductor[i].reduce())\n",
    "    bases.append(reductor[i].bases['RB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 300\n",
    "test_set = []\n",
    "for i,a in enumerate(alpha):\n",
    "    test_set.append(parameter_space[i].sample_randomly(n_test))\n",
    "    \n",
    "test_sample = fom[0].parameters.space(par_bounds).sample_randomly(n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the test absolute and relativ test error estimation as well as the exact error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_err_est = []\n",
    "test_err_est_rel = []\n",
    "test_err = []\n",
    "\n",
    "\n",
    "for i,a in enumerate(alpha):\n",
    "        test_err_est.append(np.zeros(len(bases[i])))\n",
    "        test_err.append(np.zeros(len(bases[i])))\n",
    "        test_err_est_rel.append(np.zeros(len(bases[i])))\n",
    "\n",
    "        #determine exact sol for each alpha                                     #for the exact train error \n",
    "        sol_ex = []\n",
    "        for j in range(len(test_set[i])):\n",
    "                # sol_ex.append(fom[i].solve(test_set[i][j]))\n",
    "                sol_ex.append(fom[i].solve(test_sample[j]))\n",
    "\n",
    "        #set up reduced model for each dim = 1,...,N\n",
    "        for dim in range(len(test_err_est[i])):\n",
    "                reductor_tmp = CoerciveRBReductor(reductor[i].fom,RB = bases[i][:dim+1], product = reductor[i].products['RB'], \n",
    "                coercivity_estimator = reductor[i].coercivity_estimator,check_orthonormality=reductor[i].check_orthonormality,\n",
    "                        check_tol=reductor[i].check_tol)\n",
    "                rom_tmp = reductor_tmp.reduce()\n",
    "                \n",
    "                #compute err estimate and ex err absolute and relative\n",
    "                test_err_est_tmp = np.zeros(n_test)\n",
    "                test_err_est_rel_tmp = np.zeros(n_test)\n",
    "                test_err_tmp = np.zeros(n_test)\n",
    "                train_err_rel_tmp = np.zeros(n_test)\n",
    "\n",
    "                for j in range(len(test_set[i])):\n",
    "                        # test_err_est_tmp[j] = rom_tmp.estimate_error(test_set[i][j])          #determine train err_estimate\n",
    "                        test_err_est_tmp[j] = rom_tmp.estimate_error(test_sample[j])\n",
    "                        test_err_est_rel_tmp[j] = test_err_est_tmp[j]/np.square(reductor_tmp.products['RB'].apply2(sol_ex[j],sol_ex[j]))\n",
    "                        \n",
    "                        # sol_rb = rom_tmp.solve(test_set[i][j])\n",
    "                        sol_rb = rom_tmp.solve(test_sample[j])\n",
    "                        diff = reductor_tmp.reconstruct(sol_rb) - sol_ex[j]\n",
    "                        test_err_tmp[j] = np.square(reductor_tmp.products['RB'].apply2(diff,diff))      #determine train err_ex       \n",
    "\n",
    "                test_err_est[i][dim] = np.max(test_err_est_tmp)\n",
    "                test_err_est_rel[i][dim] = np.max(test_err_est_rel_tmp)\n",
    "                test_err[i][dim] = np.max(test_err_tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the figures, stored in 'alpha02' and 'alphas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0 = plt.figure(0)\n",
    "for i,a in enumerate(alpha):\n",
    "    plt.semilogy(range(len(bases[i])),test_err[i],label = 'truth test error')# 'r'$\\alpha = $%.2f' %(alpha[i])\n",
    "    #p =plt.semilogy(range(greedy_data[i]['extensions']+1),train_err[i],'-', label = 'test error est')#label = r'$\\alpha = $%.2f' %(alpha[i]))\n",
    "    plt.semilogy(range(len(bases[i])),test_err_est[i],label = 'test err est') #,color = p[0].get_color())\n",
    "plt.legend()\n",
    "plt.xlabel('$N$')\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "fig1 = plt.figure(1)\n",
    "for i,a in enumerate(alpha):\n",
    "    p =plt.semilogy(range(greedy_data[i]['extensions']+1),train_err[i], label = 'train error')#, label = r'$\\alpha = $%.2f' %(alpha[i]))\n",
    "    plt.semilogy(range(len(bases[i])),test_err_est[i],label = 'test error') #,color = p[0].get_color())\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlabel('$N$')\n",
    "\n",
    "\n",
    "fig2 = plt.figure(2)\n",
    "for i,a in enumerate(alpha):\n",
    "    plt.semilogy(range(len(bases[i])),test_err_est_rel[i], label = 'rel test error')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlabel('$N$')\n",
    "\n",
    "plt.figure(3)\n",
    "for i,a in enumerate(alpha):\n",
    "\n",
    "    plt.semilogy(range(len(bases[i])),test_err_est_rel[i], label = 'rel test error')\n",
    "    plt.semilogy(range(len(bases[i])),test_err_est[i],label = 'test error')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
